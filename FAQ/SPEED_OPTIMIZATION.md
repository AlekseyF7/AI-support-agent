# Оптимизация скорости ответов бота

## Проблемы и решения

### Проблема 1: Медленные ответы

**Причины:**
- Слишком мощная модель (требует много времени)
- Слишком много документов в контексте
- Длинные ответы от модели

**Решения:**

#### 1. Использовать более легкую модель

В `.env` измените:
```env
OLLAMA_MODEL=mistral  # Вместо llama2:70b или mixtral
```

Или еще легче:
```env
OLLAMA_MODEL=phi  # Самая быстрая
```

#### 2. Уменьшить количество документов в контексте

В `.env` добавьте:
```env
RAG_K_DOCS=2  # Вместо 3 (по умолчанию)
```

Меньше документов = быстрее обработка.

#### 3. Использовать более быструю модель для классификации

Классификатор уже оптимизирован для скорости.

---

### Проблема 2: Ответы на английском

**Причина:** Модель не всегда следует инструкциям о языке.

**Решение:** Улучшены промпты с явными инструкциями на русском языке.

**Что сделано:**
- Добавлены явные инструкции "Отвечай ТОЛЬКО на русском языке"
- Улучшены системные промпты
- Добавлены напоминания в пользовательских промптах

---

## Быстрые настройки

### Для максимальной скорости:

1. **Используйте легкую модель:**
   ```env
   OLLAMA_MODEL=phi
   ```

2. **Уменьшите количество документов:**
   ```env
   RAG_K_DOCS=1
   ```

3. **Используйте более быстрый компьютер** (если возможно)

### Для баланса скорости и качества:

1. **Используйте mistral:**
   ```env
   OLLAMA_MODEL=mistral
   ```

2. **Стандартные настройки:**
   ```env
   RAG_K_DOCS=2
   ```

---

## Оптимизации в коде

### Уже применено:

1. **Ограничение длины ответа:**
   - `num_predict=512` для RAG
   - `num_predict=256` для классификации

2. **Низкая температура:**
   - `temperature=0.1` для быстрых и детерминированных ответов

3. **Меньше документов в контексте:**
   - По умолчанию 2 вместо 3

4. **Улучшенные промпты:**
   - Явные инструкции на русском
   - Требование краткости

---

## Проверка скорости

### Тест производительности:

1. Запустите бота
2. Отправьте тестовое сообщение
3. Засеките время ответа

**Ожидаемое время:**
- **phi**: 2-5 секунд
- **mistral**: 5-10 секунд
- **llama2:13b**: 10-20 секунд
- **mixtral**: 15-30 секунд
- **llama2:70b**: 30-60+ секунд

---

## Дополнительные оптимизации

### 1. Использование GPU

Если у вас есть NVIDIA GPU, Ollama автоматически использует его:
- Ускоряет работу в 2-5 раз
- Особенно заметно на больших моделях

### 2. Закрытие других программ

Освободите RAM:
- Закройте браузер с множеством вкладок
- Закройте другие тяжелые программы
- Оставьте минимум 8GB свободной RAM

### 3. Использование SSD

Если модель на SSD:
- Быстрее загрузка модели
- Быстрее работа

### 4. Кэширование ответов

Можно добавить кэширование часто задаваемых вопросов (требует доработки кода).

---

## Рекомендуемые настройки

### Для быстрой работы:

```env
LLM_PROVIDER=ollama
OLLAMA_MODEL=phi
RAG_K_DOCS=1
```

### Для баланса:

```env
LLM_PROVIDER=ollama
OLLAMA_MODEL=mistral
RAG_K_DOCS=2
```

### Для качества (медленнее):

```env
LLM_PROVIDER=ollama
OLLAMA_MODEL=llama2:13b
RAG_K_DOCS=3
```

---

## Мониторинг производительности

### Проверка времени ответа:

В логах бота можно увидеть время обработки. Если нужно, можно добавить более детальное логирование.

### Оптимизация промптов:

Промпты уже оптимизированы для:
- Краткости ответов
- Русского языка
- Быстрой обработки

---

## Если все еще медленно

1. **Проверьте модель:**
   ```powershell
   ollama list
   ```
   Убедитесь, что используете правильную модель.

2. **Проверьте RAM:**
   - Откройте Диспетчер задач
   - Проверьте использование памяти
   - Если RAM загружена, закройте другие программы

3. **Проверьте CPU/GPU:**
   - Убедитесь, что Ollama использует GPU (если доступен)
   - Проверьте загрузку процессора

4. **Попробуйте другую модель:**
   - Если используете llama2:70b, попробуйте mixtral
   - Если используете mixtral, попробуйте mistral
   - Если используете mistral, попробуйте phi

---

## Итоговые рекомендации

**Для большинства случаев:**
- Модель: `mistral`
- RAG_K_DOCS: `2`
- Это даст хороший баланс скорости и качества

**Для максимальной скорости:**
- Модель: `phi`
- RAG_K_DOCS: `1`
- Быстрые ответы, но немного хуже качество

**Для лучшего качества:**
- Модель: `llama2:13b` или `mixtral`
- RAG_K_DOCS: `3`
- Лучшее качество, но медленнее

