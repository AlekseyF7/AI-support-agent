"""
–£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ –°–±–µ—Ä–±–∞–Ω–∫–∞
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç: FAQ, —Å—Ç–∞—Ç—å–∏ –ø–æ–º–æ—â–∏, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–¥—É–∫—Ç–∞—Ö
–í—ã—Ö–æ–¥–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã: JSON, ChromaDB (RAG)
"""

import asyncio
import json
import logging
import hashlib
import re
import argparse
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Set
from urllib.parse import urljoin, urlparse
from dataclasses import dataclass, asdict

# HTTP –∫–ª–∏–µ–Ω—Ç
import httpx

# –ü–∞—Ä—Å–∏–Ω–≥ HTML
from bs4 import BeautifulSoup

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π Playwright –¥–ª—è JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞
try:
    from playwright.async_api import async_playwright
    PLAYWRIGHT_AVAILABLE = True
except ImportError:
    PLAYWRIGHT_AVAILABLE = False

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å RAG
try:
    from rag_system import RAGSystem
    RAG_AVAILABLE = True
except ImportError:
    RAG_AVAILABLE = False

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(levelname)-8s | %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("SberParser")


@dataclass
class ParsedItem:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–ø–∞—Ä—Å–µ–Ω–Ω–æ–≥–æ —ç–ª–µ–º–µ–Ω—Ç–∞"""
    title: str
    content: str
    url: str
    category: str = "general"
    parsed_at: str = ""
    
    def __post_init__(self):
        if not self.parsed_at:
            self.parsed_at = datetime.now().isoformat()
    
    @property
    def id(self) -> str:
        return hashlib.md5(f"{self.title}:{self.url}".encode()).hexdigest()


class SberParser:
    """
    –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–π –ø–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ –°–±–µ—Ä–±–∞–Ω–∫–∞
    
    –†–µ–∂–∏–º—ã —Ä–∞–±–æ—Ç—ã:
    - simple: httpx (–±—ã—Å—Ç—Ä–æ, –Ω–æ –±–µ–∑ JS)
    - browser: Playwright (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —Ä–µ–Ω–¥–µ—Ä–∏—Ç JS)
    """
    
    # –°—Ç–∞—Ä—Ç–æ–≤—ã–µ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
    DEFAULT_URLS = [
        "https://www.sberbank.ru/ru/person/help",
        "https://www.sberbank.ru/ru/person/help/sberbank_online",
        "https://www.sberbank.ru/ru/person/help/cards",
        "https://www.sberbank.ru/ru/person/help/sbp",
        "https://www.sberbank.ru/ru/person/help/contributions_faq",
        "https://www.sberbank.ru/ru/person/help/consumer_faq",
        "https://www.sberbank.ru/ru/person/help/ccards_faq",
    ]
    
    HEADERS = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
        "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
        "Accept-Encoding": "gzip, deflate, br",
        "DNT": "1",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }
    
    ALLOWED_DOMAINS = {"www.sberbank.ru", "sberbank.ru"}
    
    def __init__(
        self,
        urls: Optional[List[str]] = None,
        use_browser: bool = False,
        max_depth: int = 2,
        output_file: Optional[str] = None,
        save_to_rag: bool = False
    ):
        self.urls = urls or self.DEFAULT_URLS
        self.use_browser = use_browser and PLAYWRIGHT_AVAILABLE
        self.max_depth = max_depth
        self.output_file = output_file
        self.save_to_rag = save_to_rag and RAG_AVAILABLE
        
        self.visited: Set[str] = set()
        self.results: List[ParsedItem] = []
        
        if use_browser and not PLAYWRIGHT_AVAILABLE:
            logger.warning("‚ö†Ô∏è Playwright –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è httpx.")
        
        if save_to_rag and not RAG_AVAILABLE:
            logger.warning("‚ö†Ô∏è RAG —Å–∏—Å—Ç–µ–º–∞ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ JSON.")
    
    def _is_valid_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ URL"""
        parsed = urlparse(url)
        if parsed.netloc not in self.ALLOWED_DOMAINS:
            return False
        if url in self.visited:
            return False
        # –ò—Å–∫–ª—é—á–∞–µ–º —Ñ–∞–π–ª—ã
        skip_ext = ('.pdf', '.apk', '.exe', '.zip', '.png', '.jpg', '.jpeg', '.gif', '.svg')
        if any(url.lower().endswith(ext) for ext in skip_ext):
            return False
        return True
    
    def _normalize_url(self, url: str, base_url: str) -> str:
        """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è URL"""
        absolute = urljoin(base_url, url)
        # –£–±–∏—Ä–∞–µ–º —è–∫–æ—Ä—è –∏ trailing slash
        return absolute.split('#')[0].rstrip('/')
    
    async def _fetch_simple(self, url: str) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ httpx"""
        async with httpx.AsyncClient(
            verify=False,
            headers=self.HEADERS,
            follow_redirects=True,
            timeout=15
        ) as client:
            try:
                response = await client.get(url)
                if response.status_code == 200:
                    return response.text
                logger.warning(f"HTTP {response.status_code}: {url}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {e}")
        return None
    
    async def _fetch_browser(self, url: str, browser_context) -> Optional[str]:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ Playwright"""
        page = await browser_context.new_page()
        try:
            response = await page.goto(url, wait_until="domcontentloaded", timeout=30000)
            if not response or response.status >= 400:
                return None
            
            # –ñ–¥—ë–º –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            try:
                await page.wait_for_selector("h1, h2, .kit-accordion", timeout=10000)
            except:
                pass
            
            # –°–∫—Ä–æ–ª–ª–∏–º –¥–ª—è lazy loading
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight / 2)")
            await asyncio.sleep(1)
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
            await asyncio.sleep(1)
            
            return await page.content()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –±—Ä–∞—É–∑–µ—Ä–∞ {url}: {e}")
            return None
        finally:
            await page.close()
    
    def _parse_html(self, html: str, url: str) -> List[ParsedItem]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ HTML"""
        if not html:
            return []
        
        soup = BeautifulSoup(html, 'html.parser')
        items = []
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ URL
        category = "general"
        if "/cards" in url or "/ccards" in url:
            category = "cards"
        elif "/sbp" in url:
            category = "sbp"
        elif "/sberbank_online" in url:
            category = "online"
        elif "/contributions" in url:
            category = "deposits"
        elif "/consumer" in url:
            category = "credits"
        
        # === –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: Kit-–∞–∫–∫–æ—Ä–¥–µ–æ–Ω—ã –°–±–µ—Ä–∞ ===
        accordions = soup.find_all(
            ['div', 'section', 'details'],
            class_=lambda x: x and any(k in str(x).lower() for k in ['accordion', 'faq', 'kit-details', 'help-item'])
        )
        
        for acc in accordions:
            # –ó–∞–≥–æ–ª–æ–≤–æ–∫
            title_elem = acc.find(
                ['h2', 'h3', 'h4', 'button', 'summary', 'span', 'div'],
                class_=lambda x: x and any(k in str(x).lower() for k in ['title', 'header', 'question', 'trigger', 'heading', 'summary'])
            )
            # –ö–æ–Ω—Ç–µ–Ω—Ç
            content_elem = acc.find(
                ['div', 'section', 'p'],
                class_=lambda x: x and any(k in str(x).lower() for k in ['content', 'body', 'answer', 'text', 'pane'])
            )
            
            if title_elem and content_elem:
                title = title_elem.get_text(separator=' ', strip=True)
                content = content_elem.get_text(separator=' ', strip=True)
                
                if 5 < len(title) < 500 and len(content) > 20:
                    items.append(ParsedItem(
                        title=title,
                        content=content,
                        url=url,
                        category=category
                    ))
        
        # === –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ó–∞–≥–æ–ª–æ–≤–∫–∏ + —Å–ª–µ–¥—É—é—â–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç ===
        if not items:
            for header in soup.find_all(['h2', 'h3']):
                title = header.get_text(strip=True)
                if len(title) < 10 or len(title) > 400:
                    continue
                
                content_parts = []
                for sibling in header.find_next_siblings():
                    if sibling.name in ['h1', 'h2', 'h3']:
                        break
                    text = sibling.get_text(separator=' ', strip=True)
                    if text:
                        content_parts.append(text)
                
                content = "\n".join(content_parts).strip()
                if len(content) > 30:
                    items.append(ParsedItem(
                        title=title,
                        content=content,
                        url=url,
                        category=category
                    ))
        
        # === –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏ ===
        if not items:
            for ul in soup.find_all('ul', class_=lambda x: x and 'list' in str(x).lower()):
                for li in ul.find_all('li', recursive=False):
                    text = li.get_text(separator=' ', strip=True)
                    if 20 < len(text) < 1000:
                        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–¥–µ–ª–∏—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç
                        parts = re.split(r'[:\?\-‚Äì‚Äî]', text, maxsplit=1)
                        if len(parts) == 2 and len(parts[0]) > 5 and len(parts[1]) > 10:
                            items.append(ParsedItem(
                                title=parts[0].strip(),
                                content=parts[1].strip(),
                                url=url,
                                category=category
                            ))
        
        return items
    
    def _extract_links(self, html: str, base_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ–±—Ö–æ–¥–∞"""
        soup = BeautifulSoup(html, 'html.parser')
        links = []
        
        for a in soup.find_all('a', href=True):
            href = a['href']
            absolute = self._normalize_url(href, base_url)
            
            # –¢–æ–ª—å–∫–æ —Å—Å—ã–ª–∫–∏ –Ω–∞ —Ä–∞–∑–¥–µ–ª—ã –ø–æ–º–æ—â–∏
            if self._is_valid_url(absolute) and '/help' in absolute.lower():
                links.append(absolute)
        
        return links
    
    async def _crawl_simple(self, url: str, depth: int = 0):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ —á–µ—Ä–µ–∑ httpx"""
        if depth > self.max_depth or not self._is_valid_url(url):
            return
        
        self.visited.add(url)
        logger.info(f"üìÑ [{depth}] {url}")
        
        html = await self._fetch_simple(url)
        if not html:
            return
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫—É —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞–º–∏
        if "gosuslugi.ru/crt" in html or "–ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –£–¶" in html:
            logger.warning(f"üõë –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–º: {url}")
            return
        
        items = self._parse_html(html, url)
        if items:
            logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            self.results.extend(items)
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥
        if depth < self.max_depth:
            links = self._extract_links(html, url)
            for link in links[:10]:  # –õ–∏–º–∏—Ç –Ω–∞ –∫–æ–ª-–≤–æ —Å—Å—ã–ª–æ–∫ —Å –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
                await self._crawl_simple(link, depth + 1)
    
    async def _crawl_browser(self, url: str, browser_context, depth: int = 0):
        """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥ —á–µ—Ä–µ–∑ Playwright"""
        if depth > self.max_depth or not self._is_valid_url(url):
            return
        
        self.visited.add(url)
        logger.info(f"üåê [{depth}] {url}")
        
        html = await self._fetch_browser(url, browser_context)
        if not html:
            return
        
        items = self._parse_html(html, url)
        if items:
            logger.info(f"   ‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(items)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
            self.results.extend(items)
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –æ–±—Ö–æ–¥
        if depth < self.max_depth:
            links = self._extract_links(html, url)
            for link in links[:10]:
                await self._crawl_browser(link, browser_context, depth + 1)
                await asyncio.sleep(0.5)  # –í–µ–∂–ª–∏–≤—ã–π delay
    
    async def run(self) -> List[ParsedItem]:
        """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞"""
        logger.info("=" * 50)
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –°–±–µ—Ä–±–∞–Ω–∫–∞")
        logger.info(f"   –†–µ–∂–∏–º: {'browser' if self.use_browser else 'simple'}")
        logger.info(f"   –ì–ª—É–±–∏–Ω–∞: {self.max_depth}")
        logger.info(f"   URL: {len(self.urls)} —Å—Ç–∞—Ä—Ç–æ–≤—ã—Ö")
        logger.info("=" * 50)
        
        if self.use_browser:
            async with async_playwright() as p:
                browser = await p.chromium.launch(
                    headless=True,
                    args=['--no-sandbox', '--disable-setuid-sandbox']
                )
                context = await browser.new_context(
                    viewport={'width': 1920, 'height': 1080},
                    user_agent=self.HEADERS['User-Agent']
                )
                
                for url in self.urls:
                    await self._crawl_browser(url, context)
                
                await browser.close()
        else:
            for url in self.urls:
                await self._crawl_simple(url)
        
        # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è
        seen_ids = set()
        unique_results = []
        for item in self.results:
            if item.id not in seen_ids:
                seen_ids.add(item.id)
                unique_results.append(item)
        self.results = unique_results
        
        logger.info("=" * 50)
        logger.info(f"‚ú® –ó–∞–≤–µ—Ä—à–µ–Ω–æ. –ù–∞–π–¥–µ–Ω–æ: {len(self.results)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        await self._save_results()
        
        return self.results
    
    async def _save_results(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        if not self.results:
            logger.warning("‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è")
            return
        
        # JSON —Ñ–∞–π–ª
        if self.output_file:
            output_path = Path(self.output_file)
            data = [asdict(item) for item in self.results]
            output_path.write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding='utf-8')
            logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤ {output_path}")
        
        # RAG —Å–∏—Å—Ç–µ–º–∞
        if self.save_to_rag:
            try:
                rag = RAGSystem()
                documents = [item.content for item in self.results]
                metadatas = [
                    {"question": item.title, "source": item.url, "category": item.category}
                    for item in self.results
                ]
                ids = [item.id for item in self.results]
                
                rag.collection.upsert(
                    documents=documents,
                    metadatas=metadatas,
                    ids=ids
                )
                logger.info(f"üß† –ó–∞–≥—Ä—É–∂–µ–Ω–æ –≤ RAG. –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {rag.collection.count()}")
            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ RAG: {e}")


async def main():
    parser = argparse.ArgumentParser(
        description="–ü–∞—Ä—Å–µ—Ä —Å–∞–π—Ç–∞ –°–±–µ—Ä–±–∞–Ω–∫–∞",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
–ü—Ä–∏–º–µ—Ä—ã:
  python sber_parser.py                          # –ë—ã—Å—Ç—Ä—ã–π –ø–∞—Ä—Å–∏–Ω–≥
  python sber_parser.py --browser                # –° —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º JS
  python sber_parser.py --output data.json       # –°–æ—Ö—Ä–∞–Ω–∏—Ç—å –≤ —Ñ–∞–π–ª
  python sber_parser.py --rag                    # –ó–∞–≥—Ä—É–∑–∏—Ç—å –≤ ChromaDB
  python sber_parser.py --url https://sberbank.ru/ru/person/help/cards
        """
    )
    
    parser.add_argument(
        '--url', '-u',
        action='append',
        help='URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ (–º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑)'
    )
    parser.add_argument(
        '--browser', '-b',
        action='store_true',
        help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Playwright –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ JS'
    )
    parser.add_argument(
        '--depth', '-d',
        type=int,
        default=2,
        help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≥–ª—É–±–∏–Ω–∞ –æ–±—Ö–æ–¥–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: 2)'
    )
    parser.add_argument(
        '--output', '-o',
        type=str,
        help='–ü—É—Ç—å –∫ –≤—ã—Ö–æ–¥–Ω–æ–º—É JSON —Ñ–∞–π–ª—É'
    )
    parser.add_argument(
        '--rag',
        action='store_true',
        help='–°–æ—Ö—Ä–∞–Ω–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ RAG —Å–∏—Å—Ç–µ–º—É (ChromaDB)'
    )
    
    args = parser.parse_args()
    
    scraper = SberParser(
        urls=args.url,
        use_browser=args.browser,
        max_depth=args.depth,
        output_file=args.output,
        save_to_rag=args.rag
    )
    
    results = await scraper.run()
    
    # –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω output, –≤—ã–≤–æ–¥–∏–º –ø—Ä–µ–≤—å—é
    if not args.output and not args.rag:
        print("\nüìã –ü—Ä–µ–≤—å—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
        for i, item in enumerate(results[:5], 1):
            print(f"\n{i}. [{item.category}] {item.title[:60]}...")
            print(f"   {item.content[:100]}...")
        
        if len(results) > 5:
            print(f"\n... –∏ –µ—â—ë {len(results) - 5} —ç–ª–µ–º–µ–Ω—Ç–æ–≤")


if __name__ == "__main__":
    asyncio.run(main())

