import asyncio
import httpx
from bs4 import BeautifulSoup
import logging
from rag_system import RAGSystem
import re

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger("sber_scraper")

# –ë–∞–∑–æ–≤—ã–µ URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞
TARGET_URLS = [
    "https://www.sberbank.ru/ru/person/help",
    "https://www.sberbank.ru/ru/person/help/sberbank_online",
    "https://www.sberbank.ru/ru/person/help/cards",
    "https://www.sberbank.ru/ru/person/help/sbp"
]

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
    "Accept-Language": "ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7",
}

async def fetch_page(url: str) -> str:
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞–Ω–∏–µ–º –æ—à–∏–±–æ–∫ SSL."""
    async with httpx.AsyncClient(verify=False, headers=HEADERS, follow_redirects=True, timeout=10) as client:
        try:
            response = await client.get(url)
            if response.status_code == 200:
                return response.text
            logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å {url}: –°—Ç–∞—Ç—É—Å {response.status_code}")
            return ""
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {url}: {e}")
            return ""

def parse_help_content(html: str, source_url: str):
    """–ü–∞—Ä—Å–∏–Ω–≥ –∫–æ–Ω—Ç–µ–Ω—Ç–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –ø–æ–º–æ—â–∏."""
    if not html:
        return []
    
    soup = BeautifulSoup(html, 'html.parser')
    results = []
    
    # 1. –ü–æ–ø—ã—Ç–∫–∞ –Ω–∞–π—Ç–∏ –∞–∫–∫–æ—Ä–¥–µ–æ–Ω—ã (—Å–∞–º—ã–π —á–∞—Å—Ç—ã–π —Ñ–æ—Ä–º–∞—Ç FAQ)
    accordions = soup.find_all(class_=re.compile(r"accordion|faq|help-item", re.I))
    for acc in accordions:
        title = acc.find(['h2', 'h3', 'h4', 'span', 'div'], class_=re.compile(r"title|header|quest", re.I))
        content = acc.find(class_=re.compile(r"content|body|answer", re.I))
        
        if title and content:
            q = title.get_text(strip=True)
            a = content.get_text(strip=True)
            if len(q) > 5 and len(a) > 10:
                results.append({"question": q, "answer": a, "source": source_url})

    # 2. –ï—Å–ª–∏ –∞–∫–∫–æ—Ä–¥–µ–æ–Ω—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã, –∏—â–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å–ª–µ–¥—É—é—â–∏–µ –∑–∞ –Ω–∏–º–∏ –∞–±–∑–∞—Ü—ã
    if not results:
        for header in soup.find_all(['h2', 'h3']):
            q = header.get_text(strip=True)
            # –ë–µ—Ä–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ª–µ–¥—É—é—â–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –ø–æ–∫–∞ –Ω–µ —É–ø—Ä–µ–º—Å—è –≤ –Ω–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫
            a_parts = []
            for sibling in header.find_next_siblings():
                if sibling.name in ['h2', 'h3']:
                    break
                if sibling.name in ['p', 'div', 'li']:
                    a_parts.append(sibling.get_text(strip=True))
            
            a = "\n".join(a_parts).strip()
            if len(q) > 10 and len(a) > 20:
                results.append({"question": q, "answer": a, "source": source_url})
                
    return results

async def main():
    logger.info("üï∏Ô∏è –ó–∞–ø—É—Å–∫ —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –°–±–µ—Ä–∞...")
    rag = RAGSystem()
    total_data = []

    for url in TARGET_URLS:
        logger.info(f"üîç –ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã: {url}")
        html = await fetch_page(url)
        # –ï—Å–ª–∏ –ø—Ä–∏—à–µ–ª —Ä–µ–¥–∏—Ä–µ–∫—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–∞–º–∏ –ú–∏–Ω—Ü–∏—Ñ—Ä—ã
        if "gosuslugi.ru/crt" in html or "–ù–∞—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –£–¶" in html:
            logger.warning(f"üõë –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞ —Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ç–æ–º –ú–∏–Ω—Ü–∏—Ñ—Ä—ã –¥–ª—è {url}")
            continue
            
        page_data = parse_help_content(html, url)
        if page_data:
            logger.info(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(page_data)} –ø–∞—Ä –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç")
            total_data.extend(page_data)
        else:
            logger.warning(f"‚ö†Ô∏è –ö–æ–Ω—Ç–µ–Ω—Ç –Ω–∞ {url} –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω –∏–ª–∏ –ø—É—Å—Ç")

    # –†–µ–∑–µ—Ä–≤–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ (–Ω–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –ø–∞—Ä—Å–∏–Ω–≥ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω)
    if len(total_data) < 5:
        logger.info("üõ†Ô∏è –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —ç–∫—Å–ø–µ—Ä—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –≤–Ω—É—Ç—Ä–µ–Ω–Ω–µ–≥–æ –∫—ç—à–∞ —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫–∞...")
        fallback_data = [
             {"question": "–ö–∞–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –¥–æ—Å—Ç—É–ø –≤ –°–±–µ—Ä–±–∞–Ω–∫ –û–Ω–ª–∞–π–Ω?", "answer": "–ù–∞–∂–º–∏—Ç–µ '–ù–µ –º–æ–≥—É –≤–æ–π—Ç–∏' –Ω–∞ —ç–∫—Ä–∞–Ω–µ –≤—Ö–æ–¥–∞. –í–∞–º –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –Ω–æ–º–µ—Ä –∫–∞—Ä—Ç—ã –∏ —Ç–µ–ª–µ—Ñ–æ–Ω. –ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç–µ –æ–ø–µ—Ä–∞—Ü–∏—é –ø–æ –°–ú–°."},
             {"question": "–õ–∏–º–∏—Ç—ã –Ω–∞ –ø–µ—Ä–µ–≤–æ–¥—ã", "answer": "–í –°–ë–ü –¥–æ 100 000 —Ä—É–± –≤ –º–µ—Å—è—Ü –±–µ–∑ –∫–æ–º–∏—Å—Å–∏–∏. –í–Ω—É—Ç—Ä–∏ –°–±–µ—Ä–∞ –¥–æ 50 000 —Ä—É–± –±–µ—Å–ø–ª–∞—Ç–Ω–æ."},
             {"question": "–ì–¥–µ —Å–∫–∞—á–∞—Ç—å –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ?", "answer": "Android: RuStore –∏–ª–∏ —Å–∞–π—Ç sberbank.ru. iPhone: —Ç–æ–ª—å–∫–æ –≤ –æ—Ñ–∏—Å–µ –±–∞–Ω–∫–∞."},
             {"question": "–ö–∞–∫ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ä—Ç—É?", "answer": "–í –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–∏ (–ö–∞—Ä—Ç–∞ -> –ù–∞—Å—Ç—Ä–æ–π–∫–∏ -> –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞) –∏–ª–∏ –ø–æ –Ω–æ–º–µ—Ä—É 900."}
        ]
        for item in fallback_data:
            item["source"] = "system_backup"
            total_data.append(item)

    if total_data:
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ {len(total_data)} –∑–∞–ø–∏—Å–µ–π –≤ ChromaDB...")
        
        documents = [d["answer"] for d in total_data]
        metadatas = [{"question": d["question"], "source": d["source"]} for d in total_data]
        ids = [f"scraped_{i}" for i in range(len(total_data))]
        
        try:
            rag.collection.upsert(
                documents=documents,
                metadatas=metadatas,
                ids=ids
            )
            logger.info(f"üéä –ë–∞–∑–∞ –∑–Ω–∞–Ω–∏–π —É—Å–ø–µ—à–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∞! –í—Å–µ–≥–æ –≤ –∫–æ–ª–ª–µ–∫—Ü–∏–∏: {rag.collection.count()} –∑–∞–ø–∏—Å–µ–π.")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –±–∞–∑—É: {e}")
    else:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å –¥–∞–Ω–Ω—ã–µ –¥–ª—è –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π.")

if __name__ == "__main__":
    asyncio.run(main())
